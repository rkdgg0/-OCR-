{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초성/중성/종성을 결합하는 함수\n",
    "def combine_consonants(start, middle, final):\n",
    "\n",
    "    chosung = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    jungsung = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    jongsung = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    \n",
    "    # 한글 유니코드 계산\n",
    "    final =  start*588 + middle*28 + final +44032\n",
    "\n",
    "    return chr(final) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_Hangul_consonants(path_to_image, path_to_model):\n",
    "\n",
    "    # 초성, 중성, 종성 세팅 (유니ㅐ코드 인덱스0)\n",
    "    chosung = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    jungsung = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    jongsung = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    # 모델 불러오기\n",
    "    model = tf.keras.models.load_model(path_to_model)\n",
    "    \n",
    "    # 이미지 열기\n",
    "    image = Image.open(path_to_image)\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    # 이미지 재조정\n",
    "    image = image.resize((90, 90))\n",
    "\n",
    "    # 넘파이 배열로 이미지 변환\n",
    "    image_array = np.asarray(image)\n",
    "    \n",
    "    # 픽셀값 표준화\n",
    "    image_array = [image_array.astype('float32') / 255.0]\n",
    "\n",
    "    # 리스트를 다시 넘파이 배열로 변환 : 이미지만 변환해서 도출\n",
    "    X = np.array(image_array)\n",
    "    \n",
    "    # 모델 예측\n",
    "    prediction = model.predict(X)\n",
    "\n",
    "    max_index_1 = np.argmax(prediction[0][0])\n",
    "    max_index_2 = np.argmax(prediction[1][0])\n",
    "    max_index_3 = np.argmax(prediction[2][0])\n",
    "    \n",
    "    return combine_consonants(max_index_1, max_index_2, max_index_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv8x, ResNet 모델 불러오기\n",
    "model_YOLO = YOLO('YOLOv8x_best.pt')\n",
    "model_resnet_path = 'best_ResNet_consonants_model.h5'\n",
    "\n",
    "# 영상 파일 경로 설정\n",
    "video_path = \"파일경로\"\n",
    "\n",
    "# 영상 불러오기\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "# 영상 프레임 수와 FPS 가져오기\n",
    "frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# FPS가 0인 경우 1로 설정\n",
    "if fps == 0:\n",
    "    fps = 1\n",
    "\n",
    "# 1초 단위로 이미지 추출하기\n",
    "for i in range(0, frame_count, fps):\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "    ret, frame = video.read()\n",
    "    if ret:\n",
    "        # YOLOv8 모델로 객체 탐지\n",
    "        results = model_YOLO(frame, stream=True)\n",
    "\n",
    "        # 결과 좌표구하기\n",
    "        for j, r in enumerate(results):\n",
    "            boxes = r.boxes.xyxy\n",
    "            frame_with_boxes = frame.copy()\n",
    "            \n",
    "            han_ = []\n",
    "\n",
    "            # boxes가 비어 있는 경우 루프를 건너뛰기.\n",
    "            if len(boxes) == 0:\n",
    "                continue\n",
    "            \n",
    "            boxes= sorted(boxes, key=lambda x:x[0])\n",
    "            \n",
    "            for box in boxes:\n",
    "                # YOLOv8 인식 영역 crop\n",
    "                x1, y1, x2, y2 = [int(b) for b in box]\n",
    "                cropped_image = frame_with_boxes[y1:y2, x1:x2]\n",
    "                \n",
    "                # OpenCV로 Crop된 이미지 선명도 높이기\n",
    "                alpha = 1.5  \n",
    "                beta = 30 \n",
    "                adjusted = cv2.convertScaleAbs(cropped_image, alpha=alpha, beta=beta)\n",
    "\n",
    "                kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "                emphasized = cv2.filter2D(adjusted, -1, kernel)\n",
    "                \n",
    "                alpha1 = 1.0\n",
    "                dst1 = np.clip((1+alpha1) * emphasized - 128 * alpha1, 0, 255).astype(np.uint8)\n",
    "                \n",
    "                dst = cv2.normalize(dst1, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "                cv2.imwrite('A_test.jpg', dst)\n",
    "\n",
    "                han_.append(draw_Hangul_consonants('A_test.jpg', model_resnet_path))\n",
    "\n",
    "            print(*han_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('cuda_')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d46606680d1e5075ac709ff16163917d4f6aebe691b3fa8c6b8846e596a4ceab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
